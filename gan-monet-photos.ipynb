{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-09T00:00:40.481620Z",
     "iopub.status.busy": "2024-12-09T00:00:40.480465Z",
     "iopub.status.idle": "2024-12-09T00:00:44.420564Z",
     "shell.execute_reply": "2024-12-09T00:00:44.419475Z",
     "shell.execute_reply.started": "2024-12-09T00:00:40.481575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this project I aim to use a GAN model in order to learn from the pictures presented in the data set. My aim is to create a generator and discriminator and then train the model using the Monet paintings data set. I then will run the model agianst real pictures the data in order to attempt to recreate them as paintings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Data\n",
    "\n",
    "I begin by loading in by loading in the Monet paintings data set and the photo data set. I prepare datasets for my CycleGAN by loading and preprocessing images of Monet paintings and photos. I define the directories containing the images and use glob to gather their file paths. Each image is resized and normalized to [-1,1] and batched for better processing. Using TensorFlow's AUTOTUNE, I optimize the data pipeline for faster loading and create two datasets, monet_dataset and photo_dataset, ready for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = '/kaggle/input/gan-getting-started/'\n",
    "MONET_DIR = os.path.join(DATASET_DIR, 'monet_jpg')\n",
    "PHOTO_DIR = os.path.join(DATASET_DIR, 'photo_jpg')\n",
    "\n",
    "# Image paths\n",
    "monet_images = glob(os.path.join(MONET_DIR, '*.jpg'))\n",
    "photo_images = glob(os.path.join(PHOTO_DIR, '*.jpg'))\n",
    "\n",
    "# Preprocessing\n",
    "def load_and_preprocess(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [256, 256])\n",
    "    img = (img / 127.5) - 1  # Normalize to [-1, 1]\n",
    "    return img\n",
    "\n",
    "def create_tf_dataset(image_list, batch_size=4):  # Larger batch size for speed\n",
    "    return tf.data.Dataset.from_tensor_slices(image_list).map(\n",
    "        load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "monet_dataset = create_tf_dataset(monet_images, batch_size=8)\n",
    "photo_dataset = create_tf_dataset(photo_images, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Generator\n",
    "\n",
    "\n",
    "I build a generator model for my CycleGAN to transform images. The model starts by taking an input image of size \n",
    "and processes it in three stages. First, during the downsampling stage, the image is reduced in size using convolutional layers with increasing filter sizes (64, 128, 256) to capture important features while shrinking the spatial dimensions. \n",
    "\n",
    "Next, in the bottleneck stage, the features are further compressed using a layer with 512 filters to focus on the most essential details. Finally, during the upsampling stage, the image is resized back to its original dimensions using transposed convolution layers with decreasing filter sizes (256, 128, 64), progressively restoring the details. The last layer ensures the output image has the same size as the input, with pixel values normalized between [−1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = models.Sequential(name=\"Generator\")\n",
    "    model.add(layers.Input(shape=(256, 256, 3)))\n",
    "\n",
    "    # Downsampling blocks\n",
    "    for filters in [64, 128, 256]:\n",
    "        model.add(layers.Conv2D(filters, 4, strides=2, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ReLU())\n",
    "\n",
    "    # Bottleneck block\n",
    "    model.add(layers.Conv2D(512, 4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    # Upsampling blocks\n",
    "    for filters in [256, 128, 64]:\n",
    "        model.add(layers.Conv2DTranspose(filters, 4, strides=2, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ReLU())\n",
    "\n",
    "    # Restore to 256x256x3 \n",
    "    model.add(layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "generator_monet = build_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Discriminator\n",
    "\n",
    "\n",
    "In this code, I create a discriminator model for my GAN to decide if an image is real or fake. It processes the image through layers that extract features using filters (64, 128) and gradually reduce the size of the image with strides. After each feature extraction layer, batch normalization is used to make training more stable, and Leaky ReLU activation is applied to help the model learn better by allowing small gradients even for negative values.\n",
    "\n",
    "At the end, the model outputs a single-channel result where each value represents whether a part of the image is likely real or fake. This discriminator is essential for helping the GAN distinguish between actual Monet paintings and generated ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = models.Sequential(name=\"Discriminator\")\n",
    "    model.add(layers.Input(shape=(256, 256, 3)))\n",
    "    for filters in [64, 128]:\n",
    "        model.add(layers.Conv2D(filters, 4, strides=2, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(negative_slope=0.2))  # Replace `alpha` with `negative_slope`\n",
    "    model.add(layers.Conv2D(1, 4, padding='same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss Function\n",
    "\n",
    "In this code, I define the loss functions for training my CycleGAN. The generator_loss measures how well the generator fools the discriminator by comparing its predictions on fake images to the target \"real.\" The discriminator_loss evaluates how well the discriminator distinguishes real images from fake ones, combining losses for both real and fake predictions. The cycle_loss ensures the generator preserves the original image's structure by penalizing differences between the input image and the reconstructed image, scaled by a factor of 5. These losses work together to help the CycleGAN create realistic Monet-style images while keeping the content of the original photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return loss_fn(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = loss_fn(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = loss_fn(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def cycle_loss(real_image, cycled_image):\n",
    "    return tf.reduce_mean(tf.abs(real_image - cycled_image)) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step\n",
    "\n",
    "This code trains the CycleGAN by updating the generator and discriminator models. The generator creates fake images and reconstructs original ones, while the discriminator evaluates real and fake images. Losses are calculated to improve the generator’s ability to fool the discriminator and maintain the input image’s structure. Gradients are computed and applied using optimizers to adjust the models during training efficiently. The @tf.function speeds up execution by optimizing the code as a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.6)  # Lower learning rate\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.6)\n",
    "discriminator_monet = build_discriminator()\n",
    "\n",
    "# Training step\n",
    "@tf.function  \n",
    "def train_step(real_monet, real_photo):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_photo = generator_monet(real_monet, training=True)\n",
    "        cycled_monet = generator_monet(fake_photo, training=True)\n",
    "\n",
    "        real_monet_preds = discriminator_monet(real_monet, training=True)\n",
    "        fake_monet_preds = discriminator_monet(fake_photo, training=True)\n",
    "\n",
    "        g_loss = generator_loss(fake_monet_preds) + cycle_loss(real_monet, cycled_monet)\n",
    "        d_loss = discriminator_loss(real_monet_preds, fake_monet_preds)\n",
    "\n",
    "    gen_grads = tape.gradient(g_loss, generator_monet.trainable_variables)\n",
    "    disc_grads = tape.gradient(d_loss, discriminator_monet.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gen_grads, generator_monet.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(disc_grads, discriminator_monet.trainable_variables))\n",
    "    return g_loss, d_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "For training, I use 5 epochs. For each epoch, it calculates the generator and discriminator losses over all batches, averages them, and records the time taken to complete the epoch. The generator loss reflects how well the generator creates convincing fake images, while the discriminator loss measures how effectively the discriminator distinguishes real from fake images.\n",
    "\n",
    "Based on the printed results, the model improves gradually over the epochs, as indicated by the decrease in both generator and discriminator losses. The generator loss decreases from 2.0399 to 1.6986, showing that the generator is getting better at creating realistic Monet-style images. Similarly, the discriminator loss stabilizes around 1.4, suggesting it maintains a balance in distinguishing real and fake images. The model demonstrates good progress and stability, showing that both the generator and discriminator are learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 5  \n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    start_time = time.time() \n",
    "\n",
    "    total_gen_loss = 0\n",
    "    total_disc_loss = 0\n",
    "    step_count = 0\n",
    "\n",
    "    for real_monet, real_photo in zip(monet_dataset, photo_dataset):\n",
    "        gen_loss, disc_loss = train_step(real_monet, real_photo)\n",
    "        total_gen_loss += gen_loss.numpy()\n",
    "        total_disc_loss += disc_loss.numpy()\n",
    "        step_count += 1\n",
    "\n",
    "    # average losses\n",
    "    avg_gen_loss = total_gen_loss / step_count\n",
    "    avg_disc_loss = total_disc_loss / step_count\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds\")\n",
    "    print(f\"  Avg Generator Loss: {avg_gen_loss:.4f}\")\n",
    "    print(f\"  Avg Discriminator Loss: {avg_disc_loss:.4f}\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Submission\n",
    "\n",
    "Here, the model is used to generate Monet-style images from real photos using the trained generator model and saves the results. \n",
    "It processes the first 1,000 photos from the dataset, converting each photo into a Monet-style image, denormalizing the pixel values, and saving the output to a directory. For the first five images, both the input photos and their generated Monet-style counterparts are stored for visualization. \n",
    "\n",
    "A few of these images are displayed side by side using Matplotlib to provide a quick qualitative evaluation of the generator's performance. It appears the model can mimic the images, however the Monet style generated photos are not high quality. They do appear to catch the main features of the photos not the detail. This could possibly be improved by decreasing learning rate and addign more epochs to the model to make it learn more rigorously. The trade off would however be that the model would take much longer to run. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = './monet_generated'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "input_images = []  \n",
    "generated_images = []  \n",
    "\n",
    "for i, photo in tqdm(enumerate(photo_dataset.take(1000)), total=1000):  \n",
    "    generated_monet = generator_monet(photo, training=False)[0]\n",
    "    generated_monet = (generated_monet.numpy() * 127.5 + 127.5).astype(np.uint8)  \n",
    "\n",
    "    tf.keras.preprocessing.image.save_img(os.path.join(output_dir, f\"monet_{i}.jpg\"), generated_monet)\n",
    "\n",
    "    if i < 5:\n",
    "        input_images.append((photo[0].numpy() * 127.5 + 127.5).astype(np.uint8))\n",
    "        generated_images.append(generated_monet)\n",
    "\n",
    "# first 5 images\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(5):\n",
    "    # Input image\n",
    "    plt.subplot(5, 2, 2 * i + 1)\n",
    "    plt.imshow(input_images[i])\n",
    "    plt.title(f\"Input Photo {i + 1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.subplot(5, 2, 2 * i + 2)\n",
    "    plt.imshow(generated_images[i])\n",
    "    plt.title(f\"Generated Monet {i + 1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#zip file\n",
    "with zipfile.ZipFile('images.zip', 'w') as zipf:\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), file)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 1475600,
     "sourceId": 21755,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
